{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from zipfile import ZipFile\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with ZipFile('data/9_way_dataset.zip', 'r') as zip:\n",
    "#     zip.printdir()\n",
    "#     # zip.extractall(path='/home/jipe/E208-Final-Project/data')\n",
    "#     zip.extractall()\n",
    "\n",
    "# with ZipFile('data/100_way_dataset.zip', 'r') as zip:\n",
    "#     zip.printdir()\n",
    "#     # zip.extractall(path='/home/jipe/E208-Final-Project/data')\n",
    "#     zip.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/9_way_dataset.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = data[0] + data[2] + data[4]\n",
    "labels = data[1] + data[3] + data[5]\n",
    "\n",
    "# pad images\n",
    "images = [np.append(img, np.zeros((64, 2)), axis=1) for img in images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40005"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[0], cmap=cm.Greys_r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "\n",
    "# Assuming labels is a list of string labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Now you can convert your labels to a tensor\n",
    "labels_tensor = torch.tensor(labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_134033/1356278186.py:17: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  images_tensor = torch.Tensor(images)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx][None, :, :]\n",
    "        return self.images[idx], self.labels[idx]\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "images_tensor = torch.Tensor(images)\n",
    "images_tensor = torch.stack([torch.Tensor(img[np.newaxis, ...]) for img in images])\n",
    "# labels_tensor = torch.Tensor(labels)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(images_tensor, labels_tensor, test_size=0.2)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "test_dataset = CustomDataset(X_test, y_test)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40005, 1, 64, 64])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchvision.models as models\n",
    "# import torch.nn as nn\n",
    "\n",
    "# def create_resnet_model(model_name, num_classes=9):\n",
    "#     # Load a pre-trained ResNet\n",
    "#     model = getattr(models, model_name)(pretrained=True)\n",
    "\n",
    "#     # Freeze all layers\n",
    "#     for param in model.parameters():\n",
    "#         param.requires_grad = False\n",
    "\n",
    "#     # Replace the last layer\n",
    "#     num_ftrs = model.fc.in_features\n",
    "#     model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "#     return model\n",
    "\n",
    "# # Example\n",
    "# model_resnet18 = create_resnet_model('resnet18')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "def create_resnet_model(model_name, num_classes=9):\n",
    "    # Load a pre-trained ResNet\n",
    "    model = getattr(models, model_name)(pretrained=False)\n",
    "\n",
    "    # Change the first convolutional layer to accept single-channel input\n",
    "    model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "\n",
    "    # Freeze all layers\n",
    "    # for param in model.parameters():\n",
    "    #     param.requires_grad = True\n",
    "\n",
    "    # Replace the last layer\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 501/501 [00:06<00:00, 74.44it/s]\n",
      "100%|██████████| 126/126 [00:00<00:00, 213.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Train Loss: 2.1303, Train Accuracy: 18.39%, Validation Loss: 2.1252, Validation Accuracy: 19.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 501/501 [00:06<00:00, 77.53it/s]\n",
      "100%|██████████| 126/126 [00:00<00:00, 195.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Train Loss: 2.0452, Train Accuracy: 22.69%, Validation Loss: 2.0985, Validation Accuracy: 18.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 501/501 [00:08<00:00, 60.55it/s]\n",
      "100%|██████████| 126/126 [00:00<00:00, 179.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Train Loss: 2.0210, Train Accuracy: 24.25%, Validation Loss: 2.0328, Validation Accuracy: 23.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 501/501 [00:08<00:00, 62.35it/s]\n",
      "100%|██████████| 126/126 [00:00<00:00, 178.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Train Loss: 2.0082, Train Accuracy: 24.96%, Validation Loss: 2.0907, Validation Accuracy: 20.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 501/501 [00:07<00:00, 62.90it/s]\n",
      "100%|██████████| 126/126 [00:00<00:00, 179.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Train Loss: 2.0002, Train Accuracy: 25.47%, Validation Loss: 2.0982, Validation Accuracy: 21.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example parameters\n",
    "learning_rate = 0.002\n",
    "num_epochs = 5\n",
    "\n",
    "# Choose a model\n",
    "# model = model_resnet18  # make sure this is defined as per the previous steps\n",
    "model = create_resnet_model('resnet18')\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training and Validation loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in tqdm(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_losses.append(running_loss / len(train_loader))\n",
    "    train_accuracies.append(100 * correct / total)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_losses.append(running_loss / len(test_loader))\n",
    "    val_accuracies.append(100 * correct / total)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_losses[-1]:.4f}, Train Accuracy: {train_accuracies[-1]:.2f}%, \"\n",
    "          f\"Validation Loss: {val_losses[-1]:.4f}, Validation Accuracy: {val_accuracies[-1]:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "E208_F23",
   "language": "python",
   "name": "e208_f23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
