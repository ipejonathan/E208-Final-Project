{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping tensorflow as it is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to c:\\temp\\pip-req-build-x8oju_o3\n",
      "  Resolved https://github.com/huggingface/transformers to commit acd653164b6874e395ac9d46850f67599d8cdb58\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: filelock in c:\\users\\jonathan ipe\\anaconda3\\envs\\e208\\lib\\site-packages (from transformers==4.36.0.dev0) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers==4.36.0.dev0)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.19.3 from https://files.pythonhosted.org/packages/05/09/1945ca6ba3ad8ad6e2872ba682ce8d68c5e63c8e55458ed8ab4885709f1d/huggingface_hub-0.19.4-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jonathan ipe\\anaconda3\\envs\\e208\\lib\\site-packages (from transformers==4.36.0.dev0) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jonathan ipe\\anaconda3\\envs\\e208\\lib\\site-packages (from transformers==4.36.0.dev0) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jonathan ipe\\anaconda3\\envs\\e208\\lib\\site-packages (from transformers==4.36.0.dev0) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jonathan ipe\\anaconda3\\envs\\e208\\lib\\site-packages (from transformers==4.36.0.dev0) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\jonathan ipe\\anaconda3\\envs\\e208\\lib\\site-packages (from transformers==4.36.0.dev0) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers==4.36.0.dev0)\n",
      "  Obtaining dependency information for tokenizers<0.19,>=0.14 from https://files.pythonhosted.org/packages/42/db/0061fb8004ce9173b9249a0c323c799be51f2c8e6d4ff3cc38b549c3f8b6/tokenizers-0.15.0-cp310-none-win_amd64.whl.metadata\n",
      "  Downloading tokenizers-0.15.0-cp310-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\jonathan ipe\\anaconda3\\envs\\e208\\lib\\site-packages (from transformers==4.36.0.dev0) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\jonathan ipe\\anaconda3\\envs\\e208\\lib\\site-packages (from transformers==4.36.0.dev0) (4.65.0)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.0.dev0)\n",
      "  Obtaining dependency information for fsspec>=2023.5.0 from https://files.pythonhosted.org/packages/96/0e/2be9b5a2e3f736577e749bbdf27a1e7e965041e1c908d49dedf56eeb2b8a/fsspec-2023.12.1-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2023.12.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jonathan ipe\\anaconda3\\envs\\e208\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.36.0.dev0) (4.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\jonathan ipe\\anaconda3\\envs\\e208\\lib\\site-packages (from tqdm>=4.27->transformers==4.36.0.dev0) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jonathan ipe\\anaconda3\\envs\\e208\\lib\\site-packages (from requests->transformers==4.36.0.dev0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jonathan ipe\\anaconda3\\envs\\e208\\lib\\site-packages (from requests->transformers==4.36.0.dev0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jonathan ipe\\anaconda3\\envs\\e208\\lib\\site-packages (from requests->transformers==4.36.0.dev0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jonathan ipe\\anaconda3\\envs\\e208\\lib\\site-packages (from requests->transformers==4.36.0.dev0) (2023.7.22)\n",
      "Downloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "   ---------------------------------------- 0.0/311.7 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/311.7 kB ? eta -:--:--\n",
      "   ----- --------------------------------- 41.0/311.7 kB 991.0 kB/s eta 0:00:01\n",
      "   ------------------------ --------------- 194.6/311.7 kB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 311.7/311.7 kB 2.8 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.15.0-cp310-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.1/2.2 MB 6.8 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 0.3/2.2 MB 4.0 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.4/2.2 MB 3.9 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 0.5/2.2 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.6/2.2 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 0.7/2.2 MB 3.4 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.8/2.2 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.9/2.2 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.9/2.2 MB 3.1 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.0/2.2 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.0/2.2 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.0/2.2 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.0/2.2 MB 2.3 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.1/2.2 MB 2.2 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.1/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.2/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.2/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.2/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.2/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.2/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.2/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.2/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.2/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.2/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.2/2.2 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.3/2.2 MB 1.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.3/2.2 MB 1.3 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.3/2.2 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.3/2.2 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.4/2.2 MB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.5/2.2 MB 1.3 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.5/2.2 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.6/2.2 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.6/2.2 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.6/2.2 MB 1.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.6/2.2 MB 1.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.6/2.2 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.7/2.2 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.7/2.2 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.8/2.2 MB 1.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.8/2.2 MB 1.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.8/2.2 MB 1.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.8/2.2 MB 1.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.8/2.2 MB 1.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.9/2.2 MB 1.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.9/2.2 MB 1.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.9/2.2 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.9/2.2 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.9/2.2 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.9/2.2 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.9/2.2 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.9/2.2 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.9/2.2 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.9/2.2 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.9/2.2 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.9/2.2 MB 957.4 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.9/2.2 MB 940.1 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.0/2.2 MB 943.5 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.0/2.2 MB 954.0 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.0/2.2 MB 952.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.1/2.2 MB 973.7 kB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 990.1 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 980.0 kB/s eta 0:00:00\n",
      "Downloading fsspec-2023.12.1-py3-none-any.whl (168 kB)\n",
      "   ---------------------------------------- 0.0/168.9 kB ? eta -:--:--\n",
      "   ------- -------------------------------- 30.7/168.9 kB ? eta -:--:--\n",
      "   -------------- ------------------------- 61.4/168.9 kB 1.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 92.2/168.9 kB 1.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 92.2/168.9 kB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------- ------------ 112.6/168.9 kB 726.2 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 122.9/168.9 kB 654.9 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 122.9/168.9 kB 654.9 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 122.9/168.9 kB 654.9 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 143.4/168.9 kB 500.5 kB/s eta 0:00:01\n",
      "   -------------------------------------- 168.9/168.9 kB 483.6 kB/s eta 0:00:00\n",
      "Building wheels for collected packages: transformers\n",
      "  Building wheel for transformers (pyproject.toml): started\n",
      "  Building wheel for transformers (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for transformers: filename=transformers-4.36.0.dev0-py3-none-any.whl size=8148332 sha256=8a249729afa8c7f68effe43fa14daffec421843acb3642bbbd0ba4e5f676a9b9\n",
      "  Stored in directory: C:\\Temp\\pip-ephem-wheel-cache-rd1nfzu0\\wheels\\c0\\14\\d6\\6c9a5582d2ac191ec0a483be151a4495fe1eb2a6706ca49f1b\n",
      "Successfully built transformers\n",
      "Installing collected packages: fsspec, huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.15.1\n",
      "    Uninstalling huggingface-hub-0.15.1:\n",
      "      Successfully uninstalled huggingface-hub-0.15.1\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.13.2\n",
      "    Uninstalling tokenizers-0.13.2:\n",
      "      Successfully uninstalled tokenizers-0.13.2\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.32.1\n",
      "    Uninstalling transformers-4.32.1:\n",
      "      Successfully uninstalled transformers-4.32.1\n",
      "Successfully installed fsspec-2023.12.1 huggingface-hub-0.19.4 tokenizers-0.15.0 transformers-4.36.0.dev0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers 'C:\\Temp\\pip-req-build-x8oju_o3'\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "s3fs 2023.4.0 requires fsspec==2023.4.0, but you have fsspec 2023.12.1 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'grep' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# We won't need TensorFlow here\n",
    "# %pip uninstall -y tensorflow\n",
    "# # Install `transformers` from master\n",
    "# %pip install git+https://github.com/huggingface/transformers\n",
    "# %pip list | grep -E 'transformers|tokenizers'\n",
    "# transformers version at notebook update --- 2.11.0\n",
    "# tokenizers version at notebook update --- 0.8.0rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"BPEtokenizer/vocab.json\",\n",
    "    \"BPEtokenizer/merges.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=50_000,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "tokenizer = RobertaTokenizerFast(\"BPEtokenizer/vocab.json\",\n",
    "                                 \"BPEtokenizer/merges.txt\", \n",
    "                                 max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "model = RobertaForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81966416"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()\n",
    "# => 84 million parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset text (file://C:/Users/Jonathan Ipe/.cache/huggingface/datasets/text/default-9260811a8c434195/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Loading a dataset cached in a LocalFileSystem is not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:10\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Jonathan Ipe\\anaconda3\\envs\\E208\\lib\\site-packages\\datasets\\load.py:1810\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[0;32m   1808\u001b[0m \u001b[39mif\u001b[39;00m token \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1809\u001b[0m     download_config \u001b[39m=\u001b[39m download_config\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m download_config \u001b[39melse\u001b[39;00m DownloadConfig()\n\u001b[1;32m-> 1810\u001b[0m     download_config\u001b[39m.\u001b[39mtoken \u001b[39m=\u001b[39m token\n\u001b[0;32m   1811\u001b[0m \u001b[39mif\u001b[39;00m storage_options \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1812\u001b[0m     download_config \u001b[39m=\u001b[39m download_config\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m download_config \u001b[39melse\u001b[39;00m DownloadConfig()\n",
      "File \u001b[1;32mc:\\Users\\Jonathan Ipe\\anaconda3\\envs\\E208\\lib\\site-packages\\datasets\\builder.py:1107\u001b[0m, in \u001b[0;36mas_dataset\u001b[1;34m(self, split, run_post_process, verification_mode, ignore_verifications, in_memory)\u001b[0m\n\u001b[0;32m   1105\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Get kwargs for `self._split_generators()` from `prepare_split_kwargs`.\"\"\"\u001b[39;00m\n\u001b[0;32m   1106\u001b[0m \u001b[39mdel\u001b[39;00m prepare_split_kwargs\n\u001b[1;32m-> 1107\u001b[0m \u001b[39mreturn\u001b[39;00m {}\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Loading a dataset cached in a LocalFileSystem is not supported."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# from transformers import LineByLineTextDataset\n",
    "\n",
    "# dataset = LineByLineTextDataset(\n",
    "#     tokenizer=tokenizer,\n",
    "#     file_path=r\"C:\\Users\\Jonathan Ipe\\Desktop\\unlabeled_data_sentences.txt\",\n",
    "#     block_size=128,\n",
    "# )\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"text\", data_files=r\"unlabeled_data_sentences.txt\", split='train')\n",
    "shuffled_dataset = dataset.shuffle(seed=42, buffer_size=10_000)\n",
    "\n",
    "train_dataset = shuffled_dataset.take(25_000)\n",
    "eval_dataset = shuffled_dataset.skip(25_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"BERT\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=64,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/jipe/anaconda3/envs/E208/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [313/313 02:23, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 11s, sys: 2.25 s, total: 4min 13s\n",
      "Wall time: 2min 27s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=313, training_loss=4.585713249425918, metrics={'train_runtime': 147.479, 'train_samples_per_second': 271.259, 'train_steps_per_second': 2.122, 'total_flos': 1326375701360640.0, 'train_loss': 4.585713249425918, 'epoch': 1.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"composer-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "# from tokenizers.processors import BertProcessing\n",
    "# from torch.utils.data import Dataset\n",
    "\n",
    "# class ComposerClassificationDataset(Dataset):\n",
    "#     def __init__(self, evaluate: bool = False):\n",
    "#         tokenizer = ByteLevelBPETokenizer(\n",
    "#             \"note-classifier-vocab.json\",\n",
    "#             \"note-classifier-merges.txt\",\n",
    "#         )\n",
    "#         tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "#             (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "#             (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    "#         )\n",
    "#         tokenizer.enable_truncation(max_length=512)\n",
    "#         # or use the RobertaTokenizer from `transformers` directly.\n",
    "\n",
    "#         self.examples = []\n",
    "\n",
    "#         src_files = 'sentences.txt'\n",
    "#         for src_file in src_files:\n",
    "#             print(\"🔥\", src_file)\n",
    "#             lines = src_file.read_text(encoding=\"utf-8\").splitlines()\n",
    "#             self.examples += [x.ids for x in tokenizer.encode_batch(lines)]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.examples)\n",
    "\n",
    "#     def __getitem__(self, i):\n",
    "#         # We’ll pad at the batch level.\n",
    "#         return torch.tensor(self.examples[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "E208",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "0.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
